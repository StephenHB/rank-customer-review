{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import subprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"downloading...\")\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/stephenzhang/Downloads/yelp.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract dish names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_verbs(df, col_name, top_n_verbs=30):\n",
    "    text_data_list = df[col_name].tolist()\n",
    "    verbs = []\n",
    "    for doc in nlp.pipe(text_data_list):\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\" and token.dep_ in (\"ROOT\", \"dobj\"):\n",
    "                verbs.append(token.lemma_)\n",
    "        action_verbs = set([v for v, cnt in Counter(verbs).most_common(top_n_verbs)])\n",
    "        \n",
    "    return action_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_menu_items(df, col_name, top_n_menu_items=100):\n",
    "    text_data_list = df[col_name].tolist()\n",
    "    menu_candidates = []\n",
    "    for doc in nlp.pipe(text_data_list):\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if len(chunk.text.split()) >= 2:  # 提取复合名词\n",
    "                menu_candidates.append(chunk.text.lower())\n",
    "    menu_items = set([item for item, cnt in Counter(menu_candidates).most_common(100)])\n",
    "    return menu_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_VERBS = get_action_verbs(df, 'text_clean', 20)\n",
    "menu_items = get_action_verbs(df, 'text_clean', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_dish(dish):\n",
    "    doc = nlp(dish)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dishes_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    dishes = []\n",
    "    \n",
    "    # 规则1：通过动词的直接宾语提取\n",
    "    for token in doc:\n",
    "        if token.text.lower() in ACTION_VERBS and token.dep_ == \"ROOT\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"dobj\":  # 直接宾语\n",
    "                    dish = \" \".join([w.lemma_ for w in child.subtree])\n",
    "                    dishes.append(dish)\n",
    "    \n",
    "    # 规则2：名词短语（过滤短短语）\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.text.split()) >= 2:\n",
    "            dish = \" \".join([w.lemma_ for w in chunk])\n",
    "            dishes.append(dish)\n",
    "    \n",
    "    return list(set(dishes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dishes_spacy(texts, min_freq=2):\n",
    "    all_dishes = []\n",
    "    for text in texts:\n",
    "        dishes = extract_dishes_spacy(text)\n",
    "        # 过滤停用词和通用词\n",
    "        stop_words = {\"food\", \"service\", \"restaurant\", \"place\"}\n",
    "        filtered = [\n",
    "            dish for dish in dishes\n",
    "            if not any(word in dish.split() for word in stop_words)\n",
    "        ]\n",
    "        all_dishes.extend(filtered)\n",
    "    \n",
    "    # 统计高频词\n",
    "    counter = Counter(all_dishes)\n",
    "    return [dish for dish, freq in counter.items() if freq >= min_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spacy_to_df(df):    \n",
    "    # 提取菜品并词形还原\n",
    "    df[\"dishes_raw\"] = df[\"text_clean\"].apply(extract_dishes_spacy)\n",
    "    df[\"dishes_lemmatized\"] = df[\"dishes_raw\"].apply(\n",
    "        lambda dishes: [lemmatize_dish(dish) for dish in dishes]\n",
    "    )\n",
    "    # 过滤有效菜品（基于菜单）\n",
    "    df[\"valid_dishes\"] = df[\"dishes_lemmatized\"].apply(\n",
    "        lambda dishes: [dish for dish in dishes if dish in menu_items]\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = apply_spacy_to_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
