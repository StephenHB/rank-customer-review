{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import subprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"downloading...\")\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/stephenzhang/Downloads/yelp.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract dish names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_verbs(df, col_name, top_n_verbs=30):\n",
    "    text_data_list = df[col_name].tolist()\n",
    "    verbs = []\n",
    "    for doc in nlp.pipe(text_data_list):\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\" and token.dep_ in (\"ROOT\", \"dobj\"):\n",
    "                verbs.append(token.lemma_)\n",
    "        action_verbs = set([v for v, cnt in Counter(verbs).most_common(top_n_verbs)])\n",
    "        \n",
    "    return action_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_menu_items(df, col_name, top_n_menu_items=100):\n",
    "    text_data_list = df[col_name].tolist()\n",
    "    menu_candidates = []\n",
    "    for doc in nlp.pipe(text_data_list):\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if len(chunk.text.split()) >= 2:  # 提取复合名词\n",
    "                menu_candidates.append(chunk.text.lower())\n",
    "    menu_items = set([item for item, cnt in Counter(menu_candidates).most_common(100)])\n",
    "    return menu_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_VERBS = get_action_verbs(df, 'text_clean', 20)\n",
    "menu_items = get_action_verbs(df, 'text_clean', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_dish(dish):\n",
    "    doc = nlp(dish)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dishes_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    dishes = []\n",
    "    \n",
    "    # 规则1：通过动词的直接宾语提取\n",
    "    for token in doc:\n",
    "        if token.text.lower() in ACTION_VERBS and token.dep_ == \"ROOT\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"dobj\":  # 直接宾语\n",
    "                    dish = \" \".join([w.lemma_ for w in child.subtree])\n",
    "                    dishes.append(dish)\n",
    "    \n",
    "    # 规则2：名词短语（过滤短短语）\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.text.split()) >= 2:\n",
    "            dish = \" \".join([w.lemma_ for w in chunk])\n",
    "            dishes.append(dish)\n",
    "    \n",
    "    return list(set(dishes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dishes_spacy(texts, min_freq=2):\n",
    "    all_dishes = []\n",
    "    for text in texts:\n",
    "        dishes = extract_dishes_spacy(text)\n",
    "        # 过滤停用词和通用词\n",
    "        stop_words = {\"food\", \"service\", \"restaurant\", \"place\"}\n",
    "        filtered = [\n",
    "            dish for dish in dishes\n",
    "            if not any(word in dish.split() for word in stop_words)\n",
    "        ]\n",
    "        all_dishes.extend(filtered)\n",
    "    \n",
    "    # 统计高频词\n",
    "    counter = Counter(all_dishes)\n",
    "    return [dish for dish, freq in counter.items() if freq >= min_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spacy_to_df(df, filter_validate_menue=False):    \n",
    "    # 提取菜品并词形还原\n",
    "    df[\"dishes_raw\"] = df[\"text_clean\"].apply(extract_dishes_spacy)\n",
    "    df[\"dishes_lemmatized\"] = df[\"dishes_raw\"].apply(\n",
    "        lambda dishes: [lemmatize_dish(dish) for dish in dishes]\n",
    "    )\n",
    "    if filter_validate_menue:\n",
    "        df[\"valid_dishes\"] = df[\"dishes_lemmatized\"].apply(\n",
    "            lambda dishes: [dish for dish in dishes if dish in menu_items]\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/2f0jd1bj7zgdrhk3v8rvgp1m0000gn/T/ipykernel_30663/1254182042.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df.groupby('business_id').apply(apply_spacy_to_df)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_grouped = df.groupby('business_id').apply(apply_spacy_to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>dishes_raw</th>\n",
       "      <th>dishes_lemmatized</th>\n",
       "      <th>valid_dishes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-0QBrNvhrPQCaeo7mTo0zQ</th>\n",
       "      <th>6256</th>\n",
       "      <td>-0QBrNvhrPQCaeo7mTo0zQ</td>\n",
       "      <td>2011-06-26</td>\n",
       "      <td>dx-uyV1hWZ8jQ5szxqeorg</td>\n",
       "      <td>4</td>\n",
       "      <td>Best shrimp burro's. If it wasn't in the hood ...</td>\n",
       "      <td>review</td>\n",
       "      <td>DghMl81bONCh5ELnrgn4Fw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>best shrimp burros if it wasnt in the hood  ma...</td>\n",
       "      <td>[a couple tv, your mind, the place, the staff,...</td>\n",
       "      <td>[a couple tv, your mind, the place, the staff,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0bUDim5OGuv8R0Qqq6J4A</th>\n",
       "      <th>240</th>\n",
       "      <td>-0bUDim5OGuv8R0Qqq6J4A</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>ex4pODOWrfzx1k89FEE0Kg</td>\n",
       "      <td>3</td>\n",
       "      <td>This place is busy, but the kids love the panc...</td>\n",
       "      <td>review</td>\n",
       "      <td>okcNd96gHHf_83wly93bpQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place is busy but the kids love the panca...</td>\n",
       "      <td>[so much food, the pancake, the kid, the price...</td>\n",
       "      <td>[so much food, the pancake, the kid, the price...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1N0Z3uM8xbxKS8XiAnaog</th>\n",
       "      <th>3469</th>\n",
       "      <td>-1N0Z3uM8xbxKS8XiAnaog</td>\n",
       "      <td>2009-04-20</td>\n",
       "      <td>IvTP2fHcGOG_GkwLOhqh1g</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The office space, layout, presentation gets 5...</td>\n",
       "      <td>review</td>\n",
       "      <td>IzMeF5f2043jgDjhzNeDbg</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>the office space layout presentation gets 5 st...</td>\n",
       "      <td>[the office space layout presentation, their o...</td>\n",
       "      <td>[the office space layout presentation, their o...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-34jE_5dujSWMIOBudQsiQ</th>\n",
       "      <th>6795</th>\n",
       "      <td>-34jE_5dujSWMIOBudQsiQ</td>\n",
       "      <td>2009-11-02</td>\n",
       "      <td>Ydc1XwgrFaEWKo0jR-hp1w</td>\n",
       "      <td>5</td>\n",
       "      <td>Dr. Dairiki is terrific. She is professional, ...</td>\n",
       "      <td>review</td>\n",
       "      <td>Rr1q5BfuV9u6sJX98aOgCw</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>dr dairiki is terrific she is professional lis...</td>\n",
       "      <td>[test result, professional listen, exactly wha...</td>\n",
       "      <td>[test result, professional listen, exactly wha...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-3WVw1TNQbPBzaKCaQQ1AQ</th>\n",
       "      <th>657</th>\n",
       "      <td>-3WVw1TNQbPBzaKCaQQ1AQ</td>\n",
       "      <td>2007-01-20</td>\n",
       "      <td>5vaFU2g9t88ge4Fm_JnM2A</td>\n",
       "      <td>3</td>\n",
       "      <td>Had lunch here today after hearing all the col...</td>\n",
       "      <td>review</td>\n",
       "      <td>htC49ZwXiKNka5cp0GKBfQ</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>had lunch here today after hearing all the col...</td>\n",
       "      <td>[the hot   sour soup, pizzeria bianco, the pla...</td>\n",
       "      <td>[the hot    sour soup, pizzeria bianco, the pl...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        business_id        date  \\\n",
       "business_id                                                       \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  -0QBrNvhrPQCaeo7mTo0zQ  2011-06-26   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   -0bUDim5OGuv8R0Qqq6J4A  2012-01-13   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  -1N0Z3uM8xbxKS8XiAnaog  2009-04-20   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  -34jE_5dujSWMIOBudQsiQ  2009-11-02   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   -3WVw1TNQbPBzaKCaQQ1AQ  2007-01-20   \n",
       "\n",
       "                                          review_id  stars  \\\n",
       "business_id                                                  \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  dx-uyV1hWZ8jQ5szxqeorg      4   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   ex4pODOWrfzx1k89FEE0Kg      3   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  IvTP2fHcGOG_GkwLOhqh1g      1   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  Ydc1XwgrFaEWKo0jR-hp1w      5   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   5vaFU2g9t88ge4Fm_JnM2A      3   \n",
       "\n",
       "                                                                          text  \\\n",
       "business_id                                                                      \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  Best shrimp burro's. If it wasn't in the hood ...   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   This place is busy, but the kids love the panc...   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  \"The office space, layout, presentation gets 5...   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  Dr. Dairiki is terrific. She is professional, ...   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   Had lunch here today after hearing all the col...   \n",
       "\n",
       "                               type                 user_id  cool  useful  \\\n",
       "business_id                                                                 \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  review  DghMl81bONCh5ELnrgn4Fw     0       0   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   review  okcNd96gHHf_83wly93bpQ     0       0   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  review  IzMeF5f2043jgDjhzNeDbg     3       3   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  review  Rr1q5BfuV9u6sJX98aOgCw     0       2   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   review  htC49ZwXiKNka5cp0GKBfQ     2       4   \n",
       "\n",
       "                             funny  \\\n",
       "business_id                          \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256      0   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240       1   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469      3   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795      0   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657       1   \n",
       "\n",
       "                                                                    text_clean  \\\n",
       "business_id                                                                      \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  best shrimp burros if it wasnt in the hood  ma...   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   this place is busy but the kids love the panca...   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  the office space layout presentation gets 5 st...   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  dr dairiki is terrific she is professional lis...   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   had lunch here today after hearing all the col...   \n",
       "\n",
       "                                                                    dishes_raw  \\\n",
       "business_id                                                                      \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  [a couple tv, your mind, the place, the staff,...   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   [so much food, the pancake, the kid, the price...   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  [the office space layout presentation, their o...   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  [test result, professional listen, exactly wha...   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   [the hot   sour soup, pizzeria bianco, the pla...   \n",
       "\n",
       "                                                             dishes_lemmatized  \\\n",
       "business_id                                                                      \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256  [a couple tv, your mind, the place, the staff,...   \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240   [so much food, the pancake, the kid, the price...   \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469  [the office space layout presentation, their o...   \n",
       "-34jE_5dujSWMIOBudQsiQ 6795  [test result, professional listen, exactly wha...   \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657   [the hot    sour soup, pizzeria bianco, the pl...   \n",
       "\n",
       "                            valid_dishes  \n",
       "business_id                               \n",
       "-0QBrNvhrPQCaeo7mTo0zQ 6256           []  \n",
       "-0bUDim5OGuv8R0Qqq6J4A 240            []  \n",
       "-1N0Z3uM8xbxKS8XiAnaog 3469           []  \n",
       "-34jE_5dujSWMIOBudQsiQ 6795           []  \n",
       "-3WVw1TNQbPBzaKCaQQ1AQ 657            []  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
